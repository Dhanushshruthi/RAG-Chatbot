{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd4QPmLC62NU",
        "outputId": "4f325414-f70c-4fda-ee33-557e6e4c46a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask me anything: what is RCNN?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context for answering the question:\n",
            "REGION BASED CONVOLUTIONAL NEURAL NETWORKS(R-CNN) PRESENTED BY: DHANUSHSHRUTHI S T CONVOLUTIONAL NEURAL NETWORKS: A Convolutional Neural Network (CNN) is a type of artificial neural network specifically designed for processing structured grid data, such as images. CNNs are particularly effective in tasks like image recognition, classification, object detection, and segmentation. CONVOLUTIONAL LAYER: ➢Imagine you have a flashlight and you're shining it over a wall covered in stickers. Each sticker has a number or prediction based on the information processed by the neural network. ➢In regular Convolutional Neural Networks (CNNs), the network looks at the whole picture to detect objects. It's like trying to find a specific item in a messy room by scanning the entire room at once. ➢Now, in Region-based Convolutional Neural Networks (R-CNNs), the network is smarter. Instead of looking at the whole picture all at once, it focuses on specific regions where objects might be located. It's like using a flashlight to search ➢Imagine you're a detective trying to find specific objects in a big picture. Fast R-CNN is your super-efficient assistant who helps you do this quickly and accurately. ➢Here's how Fast R-CNN works: 1.Spotting Clues: First, you quickly identify areas in the picture where objects might be located. These are like the places you mark as worth investigating further. 2.Investigating: Next, you zoom in on each marked area and carefully examine it to see if there's an object there. You look at the details to figure out what's what. 3.Reporting: Finally, you summarize your findings, saying what objects are where in the picture. ➢So, Fast R-CNN helps you efficiently find and identify objects in pictures, like a trusty detective's assistant. FASTER R-CNN ➢Imagine you're playing a game where you need to find hidden objects in a large picture. Faster R-CNN is like having a super-fast teammate who helps you find the objects quickly and accurately. ➢Here's how it works: 1.Scanning Quickly: Your teammate quickly scans the entire faster and more efficient. ➢So, Faster R-CNN speeds up the process of finding objects in images by quickly spotting potential locations and then focusing on those areas for a more detailed search. MASK R-CNN: ➢Mask R-CNN is like a super-efficient assistant for finding objects and outlining their shapes in pictures. 1. Spotting Objects: It quickly spots where objects might be in a picture. 2. Outlining Shapes: Then, it precisely outlines the shape of each object it finds. 3. Masking: It creates a detailed \"mask\"\n",
            "Answer: assistant for finding objects and outlining their shapes in pictures. 1. spotting objects\n",
            "Ask me anything: what is max pooling?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context for answering the question:\n",
            "parts. So, you take a small window and move it across the picture. In each window, you pick the biggest number (max pooling) or calculate the average of all the numbers (average pooling). Then, you keep only the biggest number or the average and throw away the rest. ➢For example, if you're doing max pooling and your window covers four squares with values 3, 6, 2, and 8, you'd keep only the 8 and throw away the others. This process helps reduce the size of the picture while preserving the most important features. ROI pooling does this by taking the maximum value in each resized region. For example, if you're resizing a 5x5 region, you'd pick the biggest number in that area. 4.Output: Finally, you end up with a set of fixed-size regions, each summarized by a single value. These values represent the most important features of the original regions. ➢So, ROI pooling is like standardizing and summarizing different-sized regions in images, making them easier to compare and analyze in a neural network. ROI POOLING: FAST R-CNN: a set of features. These features help the neural network understand different aspects of the image, like edges, textures, or shapes. So, the convolutional layer essentially captures patterns in the input image by sliding a filter over it and computing dot products at each position. POOLING LAYER: ➢Imagine you have a big picture made up of lots of small squares, like a puzzle. Each square represents a pixel in the image. In a pooling layer, you're trying to shrink down the picture while keeping the important you need to make them all the same size. ➢ROI (Region of Interest) pooling is like resizing these photos so they're all the same size, but in a smart way. 1.Divide into Regions: First, you choose specific regions in each photo that contain the object you're interested in. These could be different sizes and shapes. 2.Resize: Next, you resize each region to a fixed size, let's say 5x5 pixels, regardless of the original size. 3.Pooling: Now, you want to summarize the information in each region into a single value. bias. Finally, this sum goes through an activation function to produce the final decision. ➢So, a dense layer is essentially about gathering information from every input neuron, weighing that information, combining it, and making a decision based on it. OUTPUT LAYER: ➢Think of the output layer in a neural network like the final decision maker. Imagine you're in a group project, and after discussing and gathering information from different sources you need to present your findings or make a final conclusion. ➢For\n",
            "Answer: information from different sources\n",
            "Ask me anything: what is dense layer?\n",
            "Context for answering the question:\n",
            "bias. Finally, this sum goes through an activation function to produce the final decision. ➢So, a dense layer is essentially about gathering information from every input neuron, weighing that information, combining it, and making a decision based on it. OUTPUT LAYER: ➢Think of the output layer in a neural network like the final decision maker. Imagine you're in a group project, and after discussing and gathering information from different sources you need to present your findings or make a final conclusion. ➢For It also helps the network focus on the main parts of the image and ignore less important details. DENSE LAYER: ➢Think of a dense layer like a meeting room where everyone shares their opinion, and then decisions are made based on those opinions. Each person in the room represents a neuron in the layer. They share their thoughts (which are numbers), and those thoughts are weighted based on how important each person's opinion is. Then, all these weighted opinions are added up, possibly with an extra adjustment called a set of features. These features help the neural network understand different aspects of the image, like edges, textures, or shapes. So, the convolutional layer essentially captures patterns in the input image by sliding a filter over it and computing dot products at each position. POOLING LAYER: ➢Imagine you have a big picture made up of lots of small squares, like a puzzle. Each square represents a pixel in the image. In a pooling layer, you're trying to shrink down the picture while keeping the important example, let's say you're working on a project to predict whether an image contains a cat or a dog. After processing the image through the neural network, the output layer is where the network makes its final decision. If the output neuron for \"cat\" has a higher value than the neuron for \"dog,\" the network concludes that the image contains a cat. Conversely, if the neuron for \"dog\" has a higher value, the network concludes that the image contains a dog. ➢So, the output layer essentially gives you the final answer on it. When you shine the flashlight over a group of stickers, you multiply each sticker's number by a number on a transparent grid that fits over the stickers. Then you add up all these multiplied numbers. This gives you a new number, and you write it down on another wall. ➢In a convolutional layer of a neural network, the stickers are the pixels in a small section of an image, the transparent grid is called a filter or kernel, and the new number is a feature. This process repeats across the entire image, creating\n",
            "Answer: meeting room where everyone shares their opinion\n",
            "Ask me anything: exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load SentenceTransformer model for embedding\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load BERT model for question answering\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts all text from a PDF file.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()  # Extract text from each page\n",
        "    return text\n",
        "\n",
        "def preprocess_pdf_text(pdf_text, chunk_size=512):\n",
        "    \"\"\"\n",
        "    Splits the PDF text into chunks of specified size.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    words = pdf_text.split()\n",
        "    chunk = []\n",
        "    for word in words:\n",
        "        chunk.append(word)\n",
        "        if len(\" \".join(chunk)) > chunk_size:\n",
        "            chunks.append(\" \".join(chunk))\n",
        "            chunk = []\n",
        "    if chunk:\n",
        "        chunks.append(\" \".join(chunk))  # Add any remaining text\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings(document_chunks):\n",
        "    \"\"\"\n",
        "    Generates embeddings for each document chunk.\n",
        "    \"\"\"\n",
        "    return embedding_model.encode(document_chunks)\n",
        "\n",
        "def create_faiss_index(doc_embeddings):\n",
        "    \"\"\"\n",
        "    Creates a FAISS index for similarity search.\n",
        "    \"\"\"\n",
        "    dimension = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
        "    index.add(np.array(doc_embeddings))   # Add embeddings to the index\n",
        "    return index\n",
        "\n",
        "def retrieve_documents(query, index, document_chunks, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant document chunks based on a query.\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    selected_chunks = [document_chunks[i] for i in indices[0]]\n",
        "\n",
        "    # Combine the chunks into a single context\n",
        "    context = \" \".join(selected_chunks)\n",
        "\n",
        "    return context\n",
        "\n",
        "def split_long_context(context, max_length=512):\n",
        "    \"\"\"\n",
        "    Splits a long context into smaller chunks that fit within the max token length.\n",
        "    \"\"\"\n",
        "    tokens = bert_tokenizer.encode(context)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), max_length - 2):  # -2 for special tokens\n",
        "        chunk = tokens[i:i + max_length - 2]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def prepare_input_for_bert(question, context, max_length=512):\n",
        "    \"\"\"\n",
        "    Prepares input for the BERT model, truncating the context if necessary.\n",
        "    \"\"\"\n",
        "    encoded = bert_tokenizer.encode_plus(\n",
        "        question, context,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_length,  # Truncate to the max token limit\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def generate_answer_with_bert(question, context, max_length=512):\n",
        "    \"\"\"\n",
        "    Generates an answer using BERT based on the question and context.\n",
        "    \"\"\"\n",
        "    encoded = prepare_input_for_bert(question, context, max_length)\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "    attention_mask = encoded[\"attention_mask\"]\n",
        "\n",
        "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Extract start and end logits\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # Get the start and end positions for the answer (Top 3 instead of just the highest)\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores)\n",
        "\n",
        "    # Extract the answer tokens and decode them\n",
        "    answer_tokens = input_ids[0][start_index:end_index + 1]\n",
        "    answer = bert_tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "def generate_answer_with_bert_multiple_chunks(question, context, max_length=512):\n",
        "    \"\"\"\n",
        "    Generate an answer by using multiple chunks if the context is too long.\n",
        "    \"\"\"\n",
        "    chunks = split_long_context(context, max_length)\n",
        "    answers = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_text = bert_tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "        answer = generate_answer_with_bert(question, chunk_text, max_length)\n",
        "        answers.append(answer)\n",
        "\n",
        "    # Combine all answers, prioritizing the most relevant one\n",
        "    return max(answers, key=len)  # You can adjust this logic to rank answers by relevance\n",
        "\n",
        "def generate_answer(query, index, document_chunks):\n",
        "    relevant_docs = retrieve_documents(query, index, document_chunks, top_k=5)\n",
        "    print(\"Context for answering the question:\")\n",
        "    print(relevant_docs)  # Check the combined context\n",
        "\n",
        "    # Handle long context by splitting if needed\n",
        "    answer = generate_answer_with_bert_multiple_chunks(query, relevant_docs)\n",
        "\n",
        "    return answer\n",
        "\n",
        "def chatbot(pdf_path):\n",
        "    # Extract text from the PDF\n",
        "    pdf_text = extract_text_from_pdf(pdf_path)\n",
        "    document_chunks = preprocess_pdf_text(pdf_text)\n",
        "\n",
        "    # Generate embeddings for the document chunks\n",
        "    doc_embeddings = generate_embeddings(document_chunks)\n",
        "\n",
        "    # Create the FAISS index for similarity search\n",
        "    index = create_faiss_index(doc_embeddings)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Ask me anything: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        answer = generate_answer(user_input, index, document_chunks)\n",
        "        print(f\"Answer: {answer}\")\n",
        "\n",
        "# Run the chatbot with a PDF document\n",
        "chatbot(\"/content/drive/MyDrive/R-CNN PPT.pdf\")\n"
      ]
    }
  ]
}